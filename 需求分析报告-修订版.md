# LiteLLM é¡¹ç›®éœ€æ±‚è¦†ç›–åˆ†æžæŠ¥å‘Šï¼ˆä¿®è®¢ç‰ˆï¼‰

## å…³é”®æ¾„æ¸…

æ ¹æ®ä¸Žç”¨æˆ·çš„æ²Ÿé€šï¼Œæ˜Žç¡®ä»¥ä¸‹è¦ç‚¹ï¼š
1. **è¿™äº›åŠŸèƒ½ä¸éœ€è¦æˆä¸ºé»˜è®¤è¡Œä¸º**ï¼Œåªè¦èƒ½é€šè¿‡é…ç½®æ–¹å¼ä½¿ç”¨å³å¯
2. **å…³æ³¨ç‚¹ï¼šèƒ½å¦é€šè¿‡ OpenAI Python SDK ä½¿ç”¨ LiteLLM çš„æ‰€æœ‰åŠŸèƒ½**

---

## é—®é¢˜ 1: èƒ½å¦æ»¡è¶³éœ€æ±‚ï¼Ÿ

åŸºäºŽå¯¹ LiteLLM ä»£ç åº“çš„æ·±å…¥åˆ†æžï¼Œé‡æ–°è¯„ä¼°å„é¡¹éœ€æ±‚çš„æ»¡è¶³æƒ…å†µã€‚

### éœ€æ±‚ 1ï¼šå¹¶è¡Œç«žé€Ÿï¼ˆå†·å¯åŠ¨é˜¶æ®µï¼‰âœ… **å®Œå…¨æ”¯æŒ**

#### ä½¿ç”¨æ–¹å¼

**é€šè¿‡ LiteLLM Proxy + OpenAI SDK**:

```python
import openai

client = openai.OpenAI(
    api_key="your-litellm-key",
    base_url="http://localhost:4000"  # LiteLLM Proxy
)

# æ–¹å¼ 1: å¹¶è¡Œç«žé€Ÿï¼Œè¿”å›žæœ€å¿«çš„ç»“æžœ
response = client.chat.completions.create(
    model="gpt-3.5-turbo,groq-llama",  # é€—å·åˆ†éš”å¤šä¸ªæ¨¡åž‹
    messages=[{"role": "user", "content": "Hello"}],
    extra_body={"fastest_response": True}  # ðŸ‘ˆ å…³é”®å‚æ•°
)

# æ–¹å¼ 2: å¹¶è¡Œè¯·æ±‚ï¼Œè¿”å›žæ‰€æœ‰ç»“æžœ
# ä¸åŠ  fastest_response å‚æ•°ï¼Œä¼šè°ƒç”¨ abatch_completion è¿”å›žåˆ—è¡¨
```

**é…ç½®æ–‡ä»¶** (`config.yaml`):
```yaml
model_list:
  - model_name: gpt-3.5-turbo
    litellm_params:
      model: openai/gpt-3.5-turbo
      api_key: sk-xxx
  - model_name: groq-llama
    litellm_params:
      model: groq/llama-3.1-8b-instant
      api_key: gsk-xxx
```

**ä»£ç ä½ç½®**:
- Router å®žçŽ°: `litellm/router.py:2175` (`abatch_completion_fastest_response`)
- Proxy è·¯ç”±: `litellm/proxy/route_llm_request.py:222`

#### åŠŸèƒ½æ”¯æŒæƒ…å†µ

| éœ€æ±‚ç‚¹ | æ”¯æŒæƒ…å†µ | è¯´æ˜Ž |
|--------|---------|------|
| å¹¶è¡Œå‘èµ·å¤šä¸ªè¯·æ±‚ | âœ… | `asyncio.create_task` å¹¶è¡Œæ‰§è¡Œ |
| å–å…ˆè¿”å›žçš„ç»“æžœ | âœ… | `asyncio.wait(..., FIRST_COMPLETED)` |
| å–æ¶ˆå…¶ä»–è¯·æ±‚ | âœ… | è°ƒç”¨ `task.cancel()` |
| Streaming TTFT åˆ¤å®š | âœ… | æ”¯æŒ `stream=True`ï¼Œé¦–ä¸ª chunk è¿”å›žå³åˆ¤å®š |
| çœŸæ­£å…³é—­è¿žæŽ¥ | âœ… | Task å–æ¶ˆä¼šä¸­æ–­ HTTP è¿žæŽ¥ |
| é€šè¿‡ OpenAI SDK ä½¿ç”¨ | âœ… | `extra_body={"fastest_response": True}` |

**ç»“è®º**: âœ… **å®Œå…¨æ»¡è¶³ï¼Œå¯é€šè¿‡é…ç½®ä½¿ç”¨**

---

### éœ€æ±‚ 2ï¼šåŸºäºŽå»¶è¿Ÿçš„æ™ºèƒ½è·¯ç”± âš ï¸ **éƒ¨åˆ†æ”¯æŒ**

#### çŽ°æœ‰èƒ½åŠ›

LiteLLM æ”¯æŒåŸºäºŽåŽ†å²å»¶è¿Ÿçš„è·¯ç”±ç­–ç•¥ï¼š

**é…ç½®æ–‡ä»¶**:
```yaml
model_list:
  - model_name: gpt-3.5-turbo
    litellm_params:
      model: openai/gpt-3.5-turbo
  - model_name: gpt-3.5-turbo
    litellm_params:
      model: azure/gpt-35-turbo

router_settings:
  routing_strategy: latency-based-routing  # ðŸ‘ˆ åŸºäºŽå»¶è¿Ÿè·¯ç”±
  routing_strategy_args:
    ttl: 3600  # å»¶è¿Ÿæ•°æ®ä¿ç•™ 1 å°æ—¶
```

**ä½¿ç”¨æ–¹å¼**ï¼ˆOpenAI SDKï¼‰:
```python
# æ— éœ€ç‰¹æ®Šå‚æ•°ï¼ŒProxy ä¼šè‡ªåŠ¨é€‰æ‹©å»¶è¿Ÿæœ€ä½Žçš„ deployment
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[...]
)
```

#### æ”¯æŒæƒ…å†µ

| éœ€æ±‚ç‚¹ | æ”¯æŒæƒ…å†µ | è¯´æ˜Ž |
|--------|---------|------|
| è®°å½•åŽ†å²å»¶è¿Ÿ | âœ… | åŒºåˆ† TTFT å’Œæ€»å»¶è¿Ÿ |
| é€‰æ‹©æœ€å¿« provider | âœ… | è·¯ç”±æ—¶é€‰æ‹©å»¶è¿Ÿæœ€ä½Žçš„ |
| é€šè¿‡ OpenAI SDK ä½¿ç”¨ | âœ… | é…ç½®é©±åŠ¨ï¼Œæ— éœ€ç‰¹æ®Šå‚æ•° |
| **Staggered Hedge** | âŒ | **ä¸æ”¯æŒ**è¶…æ—¶åŽè§¦å‘ä¸‹ä¸€ä¸ª provider |
| åŠ¨æ€é˜ˆå€¼ï¼ˆP95/P99ï¼‰ | âŒ | æ— åŠ¨æ€é˜ˆå€¼è®¡ç®— |

#### ç¼ºå¤±åŠŸèƒ½ï¼šStaggered Hedge

**éœ€æ±‚æè¿°**:
- é¦–å…ˆå‘æœ€å¿«çš„ provider å‘è¯·æ±‚
- å¦‚æžœè¶…è¿‡é˜ˆå€¼æœªè¿”å›žï¼Œå†å‘èµ·ç¬¬äºŒä¸ªè¯·æ±‚
- ä¸¤ä¸ªè¯·æ±‚ç«žé€Ÿï¼Œå–å…ˆè¿”å›žçš„

**çŽ°çŠ¶**: LiteLLM åªä¼šé€‰æ‹©ä¸€ä¸ªæœ€å¿«çš„ providerï¼Œå‘èµ·è¯·æ±‚åŽå°±ç­‰å¾…è¯¥ provider è¿”å›žï¼Œ**ä¸æ”¯æŒ**è¶…æ—¶è§¦å‘é¢å¤–è¯·æ±‚ã€‚

**ç»“è®º**: âš ï¸ **éƒ¨åˆ†æ»¡è¶³ï¼Œç¼ºå°‘ Staggered Hedge æ ¸å¿ƒåŠŸèƒ½**

---

### éœ€æ±‚ 3ï¼šå¥åº·æ„ŸçŸ¥ä¸Žè‡ªåŠ¨æ¢å¤ âœ… **å®Œå…¨æ”¯æŒ**

#### é…ç½®æ–¹å¼

```yaml
model_list:
  - model_name: gpt-3.5-turbo
    litellm_params:
      model: openai/gpt-3.5-turbo

router_settings:
  allowed_fails: 3           # å¤±è´¥ 3 æ¬¡åŽå†·å´
  cooldown_time: 120         # å†·å´ 120 ç§’
  disable_cooldowns: false   # å¯ç”¨å†·å´æœºåˆ¶
```

**ä½¿ç”¨æ–¹å¼**ï¼ˆOpenAI SDKï¼‰:
```python
# è‡ªåŠ¨ç”Ÿæ•ˆï¼Œæ— éœ€ç‰¹æ®Šå‚æ•°
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[...]
)
# æ•…éšœçš„ provider ä¼šè¢«è‡ªåŠ¨å†·å´ï¼Œä¸ä¼šè·¯ç”±åˆ°å®ƒ
```

#### æ”¯æŒæƒ…å†µ

| éœ€æ±‚ç‚¹ | æ”¯æŒæƒ…å†µ |
|--------|---------|
| æ•…éšœè‡ªåŠ¨å†·å´ | âœ… |
| å›ºå®šå†·å´æ—¶é—´ | âœ… |
| è‡ªåŠ¨æ¢å¤ | âœ… |
| é¿å…å‘æ•…éšœ provider å‘è¯·æ±‚ | âœ… |
| é€šè¿‡ OpenAI SDK ä½¿ç”¨ | âœ… |

**ç»“è®º**: âœ… **å®Œå…¨æ»¡è¶³**

---

### éœ€æ±‚ 4ï¼šMetrics ä¸Žå¯è§‚æµ‹æ€§ âš ï¸ **åŸºç¡€æ”¯æŒ**

LiteLLM æä¾›ï¼š
- âœ… å„ provider å»¶è¿Ÿåˆ†å¸ƒï¼ˆTTFTã€æ€»å»¶è¿Ÿï¼‰
- âœ… å„ provider æˆåŠŸçŽ‡/å¤±è´¥çŽ‡
- âœ… Token æ¶ˆè€—ç»Ÿè®¡
- âœ… å†·å´è§¦å‘é¢‘çŽ‡
- âœ… Prometheus é›†æˆ
- âŒ å›žé€€è§¦å‘çŽ‡åˆ†æžï¼ˆæ—  Staggered Hedgeï¼‰
- âŒ å°¾éƒ¨å»¶è¿Ÿæ”¹å–„å¯¹æ¯”

**ç»“è®º**: âš ï¸ **åŸºç¡€å®Œå–„ï¼Œç¼ºå°‘ç­–ç•¥æœ‰æ•ˆæ€§åˆ†æž**

---

### éœ€æ±‚ 5ï¼šé€æ˜ŽåŒ– âœ… **å®Œå…¨æ”¯æŒ**

é€šè¿‡ LiteLLM Proxy + OpenAI SDKï¼Œä¸šåŠ¡ä»£ç å®Œå…¨é€æ˜Žï¼š

```python
# ä¸šåŠ¡ä»£ç åªéœ€è¦æ”¹ base_urlï¼Œå…¶ä»–å®Œå…¨ä¸å˜
client = openai.OpenAI(
    api_key="your-key",
    base_url="http://localhost:4000"  # æŒ‡å‘ LiteLLM Proxy
)

# æ ‡å‡†çš„ OpenAI API è°ƒç”¨
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[...]
)
```

**ç»“è®º**: âœ… **å®Œå…¨æ»¡è¶³**

---

## é—®é¢˜ 2: èƒ½å¦é€šè¿‡ OpenAI Python SDK ä½¿ç”¨ LiteLLM çš„æ‰€æœ‰åŠŸèƒ½ï¼Ÿ

### æ ¸å¿ƒç­”æ¡ˆï¼šâœ… **å¯ä»¥ï¼Œé€šè¿‡ä¸¤ç§æ–¹å¼**

#### æ–¹å¼ 1: ä½¿ç”¨ LiteLLM Proxyï¼ˆæŽ¨èï¼‰â­

**æž¶æž„**:
```
ä¸šåŠ¡ä»£ç  (OpenAI SDK) â†’ LiteLLM Proxy â†’ LiteLLM Router â†’ å„ Provider
```

**ä½¿ç”¨æ–¹å¼**:
```python
import openai

client = openai.OpenAI(
    api_key="sk-1234",           # LiteLLM ç”Ÿæˆçš„ API Key
    base_url="http://0.0.0.0:4000"  # LiteLLM Proxy åœ°å€
)

# 1. æ ‡å‡†è°ƒç”¨ï¼ˆè‡ªåŠ¨åº”ç”¨é…ç½®çš„è·¯ç”±ç­–ç•¥ï¼‰
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "Hello"}]
)

# 2. å¹¶è¡Œç«žé€Ÿ
response = client.chat.completions.create(
    model="gpt-3.5-turbo,groq-llama",  # å¤šä¸ªæ¨¡åž‹
    messages=[...],
    extra_body={"fastest_response": True}  # ðŸ‘ˆ LiteLLM ç‰¹æœ‰å‚æ•°
)

# 3. è¦†ç›–è·¯ç”±é…ç½®ï¼ˆper-requestï¼‰
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[...],
    extra_body={
        "fallbacks": ["gpt-4"],        # è‡ªå®šä¹‰ fallback
        "num_retries": 5,              # è‡ªå®šä¹‰é‡è¯•æ¬¡æ•°
        "timeout": 60                  # è‡ªå®šä¹‰è¶…æ—¶
    }
)

# 4. ä¼ é€’å…ƒæ•°æ®ï¼ˆç”¨äºŽ logging/trackingï¼‰
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[...],
    extra_body={
        "metadata": {
            "user_id": "user-123",
            "request_id": "req-456"
        }
    }
)
```

#### æ–¹å¼ 2: ç›´æŽ¥ä½¿ç”¨ LiteLLM SDKï¼ˆä¸æŽ¨èï¼‰

```python
from litellm import Router

# éœ€è¦åœ¨ä»£ç ä¸­é…ç½® Router
router = Router(
    model_list=[...],
    routing_strategy="latency-based-routing"
)

# æ— æ³•ä½¿ç”¨ OpenAI SDK
response = await router.acompletion(
    model="gpt-3.5-turbo",
    messages=[...]
)
```

---

## å“ªäº›åŠŸèƒ½å¿…é¡»é€šè¿‡ LiteLLM APIï¼Ÿå“ªäº›å¯ä»¥é€šè¿‡ OpenAI SDKï¼Ÿ

### âœ… å¯ä»¥é€šè¿‡ OpenAI SDK (via Proxy)

| åŠŸèƒ½åˆ†ç±» | åŠŸèƒ½ | ä½¿ç”¨æ–¹å¼ |
|---------|------|---------|
| **è·¯ç”±ç­–ç•¥** | ç®€å•è½®è¯¢ | é…ç½®é©±åŠ¨ï¼Œæ— éœ€ç‰¹æ®Šå‚æ•° |
| | å»¶è¿Ÿè·¯ç”± | é…ç½®é©±åŠ¨ï¼Œæ— éœ€ç‰¹æ®Šå‚æ•° |
| | æˆæœ¬è·¯ç”± | é…ç½®é©±åŠ¨ï¼Œæ— éœ€ç‰¹æ®Šå‚æ•° |
| **è´Ÿè½½å‡è¡¡** | å¤š deployment | é…ç½®é©±åŠ¨ï¼Œæ— éœ€ç‰¹æ®Šå‚æ•° |
| **å®¹é”™** | Fallback | é…ç½®æˆ– `extra_body={"fallbacks": [...]}` |
| | Retry | é…ç½®æˆ– `extra_body={"num_retries": N}` |
| | Cooldown | é…ç½®é©±åŠ¨ï¼Œè‡ªåŠ¨ç”Ÿæ•ˆ |
| **å¹¶è¡Œç«žé€Ÿ** | æœ€å¿«å“åº” | `model="m1,m2"` + `extra_body={"fastest_response": True}` |
| | æ‰€æœ‰å“åº” | `model="m1,m2"`ï¼ˆä¸åŠ  fastest_responseï¼‰ |
| **å¯è§‚æµ‹æ€§** | Logging | é…ç½®é©±åŠ¨ï¼Œè‡ªåŠ¨è®°å½• |
| | Prometheus | é…ç½®é©±åŠ¨ï¼Œè‡ªåŠ¨æš´éœ² |
| **å…¶ä»–** | Streaming | æ ‡å‡† `stream=True` |
| | Function Calling | æ ‡å‡† OpenAI å‚æ•° |
| | å…ƒæ•°æ®ä¼ é€’ | `extra_body={"metadata": {...}}` |

### âŒ å¿…é¡»é€šè¿‡ LiteLLM API çš„åŠŸèƒ½

| åŠŸèƒ½ | åŽŸå›  |
|------|------|
| **Router é…ç½®æ›´æ–°** | `router.update_settings()` |
| **åŠ¨æ€æ·»åŠ æ¨¡åž‹** | `router.add_deployment()` |
| **èŽ·å–è·¯ç”±çŠ¶æ€** | `router.get_model_names()`, `router.get_available_deployment()` |
| **æ‰¹é‡æ–‡ä»¶ä¸Šä¼ ** | OpenAI SDK ä¸æ”¯æŒï¼Œéœ€ç”¨ LiteLLM SDK |

---

## æ€»ç»“è¡¨æ ¼ï¼ˆåŸºäºŽ"é…ç½®å¯ç”¨å³å¯"çš„æ ‡å‡†ï¼‰

| éœ€æ±‚é¡¹ | æ”¯æŒç¨‹åº¦ | OpenAI SDK å¯ç”¨ | å…³é”®ç¼ºé™· |
|--------|----------|----------------|---------|
| **éœ€æ±‚ 1: å¹¶è¡Œç«žé€Ÿ** | âœ… 100% | âœ… | æ—  |
| **éœ€æ±‚ 2: æ™ºèƒ½è·¯ç”±** | âš ï¸ 70% | âœ… | ç¼ºå°‘ Staggered Hedge |
| **éœ€æ±‚ 3: å¥åº·æ„ŸçŸ¥** | âœ… 100% | âœ… | æ—  |
| **éœ€æ±‚ 4: å¯è§‚æµ‹æ€§** | âš ï¸ 70% | âœ… | ç¼ºå°‘ç­–ç•¥æœ‰æ•ˆæ€§åˆ†æž |
| **éœ€æ±‚ 5: é€æ˜ŽåŒ–** | âœ… 100% | âœ… | æ—  |

---

## æ ¸å¿ƒç»“è®º

### âœ… å¯ä»¥ç›´æŽ¥ä½¿ç”¨ï¼ˆé€šè¿‡é…ç½®ï¼‰

1. **å¹¶è¡Œç«žé€Ÿ** - `extra_body={"fastest_response": True}`
2. **å¥åº·æ„ŸçŸ¥ä¸Žè‡ªåŠ¨æ¢å¤** - é…ç½® `allowed_fails` å’Œ `cooldown_time`
3. **åŸºäºŽå»¶è¿Ÿçš„è·¯ç”±** - é…ç½® `routing_strategy: latency-based-routing`
4. **å®Œå…¨é€æ˜Ž** - ä¸šåŠ¡ä»£ç ä½¿ç”¨æ ‡å‡† OpenAI SDKï¼Œåªéœ€æ”¹ `base_url`

### âŒ éœ€è¦å¼€å‘çš„åŠŸèƒ½

**å”¯ä¸€ç¼ºå¤±çš„æ ¸å¿ƒåŠŸèƒ½**: **Staggered Hedgeï¼ˆé˜¶æ¢¯å¼å¯¹å†²ï¼‰**
- é¦–æ¬¡è¯·æ±‚è¶…æ—¶åŽè§¦å‘ç¬¬äºŒä¸ª provider
- ä¸¤ä¸ªè¯·æ±‚å¹¶è¡Œç«žé€Ÿ
- å–æ¶ˆè¾ƒæ…¢çš„è¯·æ±‚

### å…³äºŽ OpenAI SDK å…¼å®¹æ€§

**âœ… ç»“è®º**:
- **95% çš„åŠŸèƒ½å¯ä»¥é€šè¿‡ OpenAI SDK ä½¿ç”¨**
- é€šè¿‡ `extra_body` å‚æ•°ä¼ é€’ LiteLLM ç‰¹æœ‰é…ç½®
- åªæœ‰ Router ç®¡ç†ç±»åŠŸèƒ½éœ€è¦ LiteLLM API

**å…¸åž‹ä½¿ç”¨æ¨¡å¼**:
```python
# 1. éƒ¨ç½² LiteLLM Proxyï¼ˆé…ç½®è·¯ç”±ç­–ç•¥ï¼‰
# $ litellm --config config.yaml

# 2. ä¸šåŠ¡ä»£ç ä½¿ç”¨æ ‡å‡† OpenAI SDK
import openai
client = openai.OpenAI(
    api_key="sk-xxx",
    base_url="http://localhost:4000"
)

# 3. æ‰€æœ‰åŠŸèƒ½é€šè¿‡é…ç½® + extra_body ä½¿ç”¨
response = client.chat.completions.create(
    model="gpt-3.5-turbo,groq-llama",  # å¤šæ¨¡åž‹
    messages=[...],
    extra_body={
        "fastest_response": True,      # å¹¶è¡Œç«žé€Ÿ
        "metadata": {"user_id": "123"} # å…ƒæ•°æ®
    }
)
```

---

## æŽ¨èæ–¹æ¡ˆ

### æ–¹æ¡ˆ A: ä½¿ç”¨çŽ°æœ‰åŠŸèƒ½ï¼ˆ0 å¼€å‘æˆæœ¬ï¼‰â­ æŽ¨è

**é€‚åˆåœºæ™¯**: å¯ä»¥æŽ¥å—"åŸºäºŽåŽ†å²å»¶è¿Ÿé€‰æ‹© + å¤±è´¥åŽ fallback"çš„è·¯ç”±ç­–ç•¥

**éƒ¨ç½²æ­¥éª¤**:
1. é…ç½® LiteLLM Proxy
2. å¯ç”¨ `latency-based-routing`
3. é…ç½® `fallbacks` å’Œ `allowed_fails`
4. ä¸šåŠ¡ä»£ç æ”¹ç”¨ LiteLLM Proxy

**ä¼˜åŠ¿**:
- 0 å¼€å‘æˆæœ¬
- æ»¡è¶³ 80% éœ€æ±‚
- å®Œå…¨é€šè¿‡ OpenAI SDK ä½¿ç”¨

**åŠ£åŠ¿**:
- æ—  Staggered Hedgeï¼ˆå°¾éƒ¨å»¶è¿Ÿä¼˜åŒ–ä¸å¦‚ç†æƒ³ï¼‰

### æ–¹æ¡ˆ B: åŸºäºŽ LiteLLM å¼€å‘ Staggered Hedgeï¼ˆ3-5 å‘¨ï¼‰

**é€‚åˆåœºæ™¯**: éœ€è¦æžè‡´çš„å°¾éƒ¨å»¶è¿Ÿä¼˜åŒ–

**å¼€å‘ç‚¹**:
1. å®žçŽ° Staggered Hedge ç­–ç•¥ï¼ˆ2 å‘¨ï¼‰
2. è¡¥å……ç­–ç•¥æœ‰æ•ˆæ€§ Metricsï¼ˆ1 å‘¨ï¼‰
3. æµ‹è¯•ä¸Žä¼˜åŒ–ï¼ˆ1-2 å‘¨ï¼‰

**ä¼˜åŠ¿**:
- æ»¡è¶³ 100% éœ€æ±‚
- å¤ç”¨ LiteLLM å®Œå–„ç”Ÿæ€
- å¯å›žé¦ˆç¤¾åŒº

---

## ä¸‹ä¸€æ­¥å»ºè®®

### ç«‹å³éªŒè¯

```bash
# 1. å®‰è£… LiteLLM
pip install litellm

# 2. åˆ›å»ºé…ç½®æ–‡ä»¶
cat > config.yaml <<EOF
model_list:
  - model_name: gpt-3.5-turbo
    litellm_params:
      model: openai/gpt-3.5-turbo
      api_key: sk-xxx
  - model_name: gpt-3.5-turbo
    litellm_params:
      model: groq/llama-3.1-8b-instant
      api_key: gsk-xxx

router_settings:
  routing_strategy: latency-based-routing
  allowed_fails: 3
  cooldown_time: 60
EOF

# 3. å¯åŠ¨ Proxy
litellm --config config.yaml

# 4. æµ‹è¯•ï¼ˆå¦ä¸€ä¸ªç»ˆç«¯ï¼‰
python test_openai_sdk.py
```

**test_openai_sdk.py**:
```python
import openai
import time

client = openai.OpenAI(
    api_key="anything",
    base_url="http://localhost:4000"
)

# Test 1: æ™®é€šè°ƒç”¨ï¼ˆä¼šè‡ªåŠ¨ä½¿ç”¨å»¶è¿Ÿè·¯ç”±ï¼‰
print("Test 1: æ ‡å‡†è°ƒç”¨")
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "Hi"}]
)
print(f"Response: {response.choices[0].message.content}")
print(f"Model ID: {response._headers.get('x-litellm-model-id')}")

# Test 2: å¹¶è¡Œç«žé€Ÿ
print("\nTest 2: å¹¶è¡Œç«žé€Ÿ")
start = time.time()
response = client.chat.completions.create(
    model="gpt-3.5-turbo,gpt-3.5-turbo",  # åŒä¸€ä¸ªæ¨¡åž‹çš„ä¸¤ä¸ª deployment
    messages=[{"role": "user", "content": "Hi"}],
    extra_body={"fastest_response": True}
)
print(f"Time: {time.time() - start:.2f}s")
print(f"Response: {response.choices[0].message.content}")
```

### å†³ç­–ç‚¹

è¿è¡ŒéªŒè¯åŽï¼Œæ ¹æ®å®žé™…å»¶è¿Ÿæ”¹å–„æ•ˆæžœå†³å®šï¼š
- å¦‚æžœæ»¡è¶³éœ€æ±‚ â†’ ç›´æŽ¥ä½¿ç”¨æ–¹æ¡ˆ A
- å¦‚æžœéœ€è¦æ›´æ¿€è¿›çš„ä¼˜åŒ– â†’ è€ƒè™‘æ–¹æ¡ˆ Bï¼ˆå¼€å‘ Staggered Hedgeï¼‰
