# LLM 调用层需求：低延迟多 Provider 路由

## 业务背景

使用多个 LLM Provider 提供同一模型的服务。不同 provider 延迟差异大，且同一 provider 的延迟波动也很大（受排队、限流、故障等影响）。用户对延迟敏感，需要在调用层实现路由策略，始终获得最低延迟。

需要调研业界成熟方案（LLM 网关、通用微服务容错策略），选择适合的技术方案来实现。

## 核心需求

### 需求 1：并行竞速（冷启动阶段）

没有历史延迟数据时，无法判断哪个 provider 更快。此时同时向所有 provider 发起请求，哪个最先返回就用哪个，立即取消其余请求。

- Streaming 场景：以 **first token（TTFT）** 返回为胜出判定点
- Non-streaming 场景：以**完整响应**返回为胜出判定点
- 取消的请求必须真正关闭连接，避免 provider 继续生成 output token 产生费用
- 只有一个 provider 时退化为直连，无额外开销

需要调研：业界 LLM 网关（LiteLLM、Portkey 等）是否支持并行竞速？通用微服务框架（如 gRPC、Envoy）的 hedge request 实现方式？

### 需求 2：基于延迟的智能路由（需求 1 的演进）

积累历史延迟数据后，不再每次都并行发所有请求（多倍 input token 费用），而是：

1. 根据历史延迟数据，选择最快的 provider 发起请求
2. 如果该 provider 在**阈值时间**内未返回（streaming 看 TTFT，non-streaming 看总延迟），说明它可能异常慢，此时再发起下一个 provider 的请求
3. 两个请求并行竞速，取先返回的，取消另一个
4. 如果第二个也超过阈值未返回，继续发起第三个，以此类推

阈值可以基于历史数据动态计算（如 P95 延迟），也可以静态配置。

这样在正常情况下只用一个请求（零额外开销），只有尾部延迟时才触发额外请求。参考 Google "The Tail at Scale" 论文的 Staggered Hedge 策略。

需要调研：业界如何确定触发阈值？是静态配置还是动态计算（P95/P99）？延迟数据的采样窗口和衰减策略是什么？

### 需求 3：健康感知与自动恢复

- Provider 故障（超时、限流、API error）时自动冷却，一段时间内不再路由到该 provider
- 冷却期过后自动恢复，重新参与路由
- 避免反复向已知故障的 provider 发请求浪费时间

需要调研：业界的冷却策略是怎样的？固定冷却时间还是指数退避？恢复后是立即全量恢复还是渐进式恢复（如先分配少量流量试探）？

### 需求 4：Metrics 与可观测性

需要能够回答以下关键问题：

**每个 provider 的表现**：
- 各 provider 的延迟分布（TTFT、总延迟）如何？
- 各 provider 的成功率和失败率是多少？
- 各 provider 消耗了多少 token（包括被取消的请求，因为这是真实成本）？

**路由策略的有效性**：
- 有多少比例的请求触发了回退（即第一个 provider 未在阈值内返回）？
- 触发回退后，第二个 provider 真正胜出的比例是多少？还是说大部分情况下第一个最终还是先返回了（说明回退策略没有实际作用）？
- 并行竞速/回退策略相比单 provider 直连，实际降低了多少尾部延迟？

**异常与成本**：
- 被取消的请求消耗了多少 token？这是回退策略的额外成本
- 冷却机制触发了多少次？每个 provider 的冷却频率和持续时间？

需要调研：业界 LLM 网关提供了哪些可观测性指标？Prometheus 指标设计的最佳实践？如何设计 label 维度既能满足分析需求又不导致指标爆炸？

## 约束条件

- 对上层业务代码完全透明，不改变调用接口
- Streaming 一旦开始输出就不能切换 provider（行业共识）
